{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # text_data = [element.get_text().replace('\\n', ' \\n ') for element in soup.find_all(['body'])]\n",
    "    text_data = [element.get_text() for element in soup.find_all(['body'])]\n",
    "\n",
    "    return \" \".join(text_data)\n",
    "\n",
    "\n",
    "url = \"https://caesar.web.engr.illinois.edu/\"\n",
    "data = scrape_website(url)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_and_split_data(data):\n",
    "    split_data = data.split('\\n')\n",
    "    # clean_data = [info.strip() for info in split_data if info.strip()]\n",
    "    return split_data\n",
    "clean_data = clean_and_split_data(data)\n",
    "for info in clean_data:\n",
    "    print(info)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model('LLM-chatbot/model/TheBloke/Llama-2-7b-Chat-GGUF/llama-2-7b-chat.Q2_K.gguf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text):\n",
    "    paragraphs = text.split('\\n')\n",
    "    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "paragraphs = split_into_paragraphs(data)\n",
    "\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    print(f\"Paragraph {i+1}:\\n{paragraph}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=\"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\")\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(paragraphs)\n",
    "len(embeddings), len(embeddings[0])\n",
    "embedded_query = embeddings_model.embed_query(\"Who is alumni?\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "class similarity:\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_similarity(embeddings, embedded_query):\n",
    "        matrix_A = np.array(embeddings)\n",
    "        matrix_B = np.array(embedded_query).reshape(1, -1)\n",
    "        similarity_matrix = cosine_similarity(matrix_A, matrix_B)\n",
    "        similarity_matrix = similarity_matrix.flatten()\n",
    "        return similarity_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def find_top_text(cos_sim, top = 5):\n",
    "        top_5_indices = np.argsort(cos_sim)[-top:]\n",
    "        # Since argsort sorts in ascending order, we reverse the array to get the top values first\n",
    "        top_5_indices = top_5_indices[::-1]\n",
    "        for i in top_5_indices:\n",
    "            print(paragraphs[i])\n",
    "        # print(top_5_indices)\n",
    "\n",
    "sim = similarity()\n",
    "cossim = sim.cos_similarity(embeddings, embedded_query)\n",
    "sim.find_top_text(cossim, 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://caesar.web.engr.illinois.edu/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "all_splits = text_splitter.split_documents(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a0db0f8f1e541ee9c5569a871cb38d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50300127f4c416286206182d02f5572"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01f0b282960d448398f073dde045fcd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2149f91c5604cca82bae6c0fc837d99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\")\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login('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')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f3742e8de9b455aa25f83f563e67424"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'TheBloke/LLaMa-7B-GGML'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/LLaMa-7B-GGML' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTheBloke/LLaMa-7B-GGML\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTheBloke/LLaMa-7B-GGML\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m                                              device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      6\u001B[0m                                              torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m                                             \u001B[38;5;66;03m#  load_in_4bit=True\u001B[39;00m\n\u001B[1;32m     10\u001B[0m                                              )\n",
      "File \u001B[0;32m~/.conda/envs/LLM/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:761\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    759\u001B[0m tokenizer_class_py, tokenizer_class_fast \u001B[38;5;241m=\u001B[39m TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[1;32m    760\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 761\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class_fast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    763\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/LLM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1838\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1832\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m   1833\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load following files from cache: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00munresolved_files\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and cannot check if these \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1834\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfiles are necessary for the tokenizer to operate.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1835\u001B[0m     )\n\u001B[1;32m   1837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(full_file_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m full_file_name \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[0;32m-> 1838\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m   1839\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load tokenizer for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load it from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the same name. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1841\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOtherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a directory \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1842\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontaining all relevant files for a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1843\u001B[0m     )\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_id, file_path \u001B[38;5;129;01min\u001B[39;00m vocab_files\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   1846\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m file_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files:\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load tokenizer for 'TheBloke/LLaMa-7B-GGML'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/LLaMa-7B-GGML' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMa-7B-GGML\",\n",
    "                                          use_auth_token=True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/LLaMa-7B-GGML\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_auth_token=True,\n",
    "                                            #  load_in_8bit=True,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an advanced assistant that excels at summarization and finding useful infomation from text scrapped from the website. \n",
      "<</SYS>>\n",
      "\n",
      "Convert the following text scrapped from a professors' personal website to sentences to answer user's questions\n",
      "\n",
      " {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an advanced assistant that excels at summarization and finding useful infomation from text scrapped from the website. \"\n",
    "instruction = \"Convert the following text scrapped from a professors' personal website to sentences to answer user's questions\\n\\n {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text = '''\n",
    "\n",
    "Matthew Caesar\n",
    "Professor\n",
    "Department of Computer Science\n",
    "University of Illinois at Urbana-Champaign\n",
    "Urbana, IL, 61801\n",
    "\n",
    "Email: caesar (at) cs (dot) illinois (dot) edu\n",
    "Office: Room 3118, Siebel Center\n",
    "Phone: 847-323-2968\n",
    "\n",
    "\n",
    "Links: [ Publications ] [ Bio ]\n",
    "\n",
    "\n",
    "I am a Professor in the Department of Computer Science at UIUC. I am also an Affiliate Professor in the Department of Electrical and Computer Engineering, an Affiliate Research Professor in the Coordinated Science Laboratory, Affiliate Professor in the School of Information Sciences, and a member of the Information Trust Institute. I currently serve as the Vice Chair of ACM SIGCOMM, and the co-chair of The Networking Channel, an online community talk series for the computer systems and networking community. I co-founded and previously served as the Chief Science Officer and President of Veriflow (sold to VMware in 2019). I received my Ph.D. in Computer Science from UC Berkeley.\n",
    "\n",
    "My research focuses on the design, analysis, and implementation of networked and distributed systems, with an emphasis on network virtualization, routing, network algorithms, systems security, and cloud services. I like taking a multi-pronged approach to system design, building systems that work well in practice but are grounded in strong theoretical principles. My recent work involves network security, network verification, and Internet of Things. For an overview of one of my projects please see this video.\n",
    "\n",
    "Question: What's the name of the professor?'''\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  According to the text scraped from the professor's personal website, the professor's name is\n",
      "Matthew Caesar.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = llm_chain.run(text)\n",
    "\n",
    "parse_text(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "import chainlit as cl\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "@cl.on_chat_start\n",
    "def main():\n",
    "  # Instantiate the chain for that user session\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "  llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "  # Store the chain in the user session\n",
    "  cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: str):\n",
    "   # Retrieve the chain from the user session\n",
    "  llm_chain = cl.user_session.get(\"llm_chain\") # type: LLMChain\n",
    "\n",
    "  # Call the chain asynchronously\n",
    "  res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "\n",
    "  # Do any post processing here\n",
    "\n",
    "  # Send the response\n",
    "  await cl.Message(content=res[\"text\"]).send()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
