
# Creating a Chatbot using LangChain, Chainlit, and Open-source LLM

## Steps and Procedures

### **Step 1: Install and import Necessary Libraries and Modules**

I initiated the project by importing the essential libraries and modules from LangChain and Chainlit. This foundational step was crucial in setting up the environment for the development of the chatbot.

```python
from langchain import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import CTransformers
from langchain.chains import RetrievalQA
import chainlit as cl
```

### **Step 2: Loading the LLM**

To load the LLM, I utilized the CTransformers class from LangChain. I selected the model parameters to align with the requirements of the project.

```python
def load_llm():
    llm = CTransformers(
        model = "TheBloke/Llama-2-7B-Chat-GGUF",
        model_type="llama",
        max_new_tokens = 512,
        temperature = 0.5
    )
    return llm
```


### **Step 3: Web Scraping**

To extract the necessary data from the website, I used WebBaseLoader to load text information from website and use Recursive Character Text Splitter to make paragraph into overlapped sentences.Then using FAISS to vectorize it.
```python
def scrape_website():
    loader = WebBaseLoader("https://caesar.web.engr.illinois.edu/")
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=40)
    docs = text_splitter.split_documents(documents)
    db = FAISS.from_documents(docs, embeddings)
    return db
```

### **Step 4: Crafting Custom Prompt Template**

In this step, I crafted a custom prompt template. The template is structured to guide the chatbot in providing precise and friendly responses to the users.

```python

custom_prompt_template = """ Use the following pieces of information to answer the user's question. When user's question include 'he' or 'him' or 'his, it refers to Matthew Caesar.
If you don't know the answer, just say that you don't know, don't try to make up an answer. Your words should be precise and friendly.

Context: {context}
Question: {question}

Only return the helpful answer below and nothing else.
Helpful answer:

"""
```

### **Step 5: Configuring the Custom Prompt**

Here, I created a function to set up a custom prompt using the PromptTemplate class from LangChain. This prompt would later facilitate the QA retrieval process by providing a structured template for the chatbot responses.

```python
def set_custom_prompt():
    prompt = PromptTemplate(template=custom_prompt_template,
                            input_variables=['context', 'question'])
    return prompt
```

### **Step 6: Constructing the Retrieval QA Chain**

In this pivotal step, I engineered a function to establish the retrieval QA chain. I meticulously configured the parameters to ensure optimal performance of the chatbot.

```python
def retrieval_qa_chain(llm, prompt, db):
    qa_chain = RetrievalQA.from_chain_type(llm=llm,
                                       chain_type='stuff',
                                       retriever=db.as_retriever(search_kwargs={'k': 2}),
                                       return_source_documents=True,
                                       chain_type_kwargs={'prompt': prompt}
                                       )
    return qa_chain
```



### **Step 7: Developing the QA Bot**

This step involved the core development of the QA bot. I integrated the previously scraped website data and segmented it into distinct sections based on specific keywords. This segmentation allowed for a more targeted and efficient retrieval process during the QA sessions.

```python
def qa_bot():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
                                       model_kwargs={'device': 'cpu'})
    
    db = scrape_website()
    llm = load_llm()
    qa_prompt = set_custom_prompt()
    qa = retrieval_qa_chain(llm, qa_prompt, db)
    return qa
```

### **Step 8: Creating the Output Function**

I then crafted a function to fetch and return the response generated by the QA bot, serving as a bridge between the bot and the users.

```python
def final_result(query):
    qa_result = qa_bot()
    response = qa_result({'query': query})
    return response
```

### **Step 9: Implementing Chainlit for Chat Interactions**

Finally, I implemented Chainlit to manage chat interactions, defining asynchronous functions to initiate the chat and respond to user messages dynamically.

```python
@cl.on_chat_start
async def start():
    chain = qa_bot()
    msg = cl.Message(content="Starting the bot...")
    await msg.send()
    msg.content = "Hi, Welcome to Matthew Caesar Page Bot. Ask me questions!"
    await msg.update()

    cl.user_session.set("chain", chain)

@cl.on_message
async def main(message):
    chain = cl.user_session.get("chain")
    cb = cl.AsyncLangchainCallbackHandler(
        stream_final_answer=True, answer_prefix_tokens=["FINAL", "ANSWER"]
    )
    cb.answer_reached = True
    res = await chain.acall(message, callbacks=[cb])
    answer = res["result"]
    sources = res["source_documents"]

    if sources:
        answer += f"\nSources:" + str(sources)
    else:
        answer += "\nNo sources found"

    await cl.Message(content=answer).send()
```

## Resources

During the development process, I extensively referred to the following resources for guidance and insights:

- [Using LangChain for Question Answering on Own Data](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed)
- [CTransformers GitHub Repository](https://github.com/marella/ctransformers)
- [LangChain Documentation on LLM Chain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain)

